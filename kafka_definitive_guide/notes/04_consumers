----------------------------------------------------------------------------
|  CHAPTER 4 - KAFKA CONSUMERS                                             |
----------------------------------------------------------------------------

- Consumers and Consumer Groups

    - An application will have a consumer, which will subscribe to a topic, and start receiving messages,
        validating them, and writing the results.  Eventually, the pace of records produced will exceed
        the rate at which you can consume and process them.


    - To scale up the number of consumers, we use 'consumer groups', which consume the same topic by
        reading from a subset of partitions.


    - One partition can only be read by a single consumer to preserve ordering, so if we have more
        consumers than partitions some will sit idle.  This is a good reason to create topics with a
        large number of partitions.


    - We can have multiple consumer groups reading the same topic.  For instance, if several applications
        want to read all messages in a topic, they can each have their own consumer group.



- Consumer Groups and Partition Rebalance

    - If consumers are added to or removed from a consumer group, partitions are rebalanced.  Rebalances
        are important for HA and scalability, but in the normal course of events they can be undesirable.


    - There are 2 types of rebalances, 'Eager' and 'Cooperative',  With Eager rebalances, all consumers
        stop consuming, give up their ownership of partitions, rejoin the consumer group, and get a
        brand new partition assignment.


    - With Cooperative rebalances, only a small subset of partitions is reassigned.  The consumer group
        leader informs all consumers to give up a subset of partitions, they those partitions are
        reassigned.


    - Consumers maintain membership in a consumer group by sending heartbeats to a broker designated as
        the 'Group Coordinator'.  If a consumer stops sending heartbeats for long enough, the 
        Coordinator assumes it dead and triggers a rebalance.



- Static Group Membership

    - By default, consumers have a transient ID that is assigned when they join a group.  This is true
        unless you configure a consumer with a unique 'group.instance.id', which makes the consumer
        a 'static member' of the group.


    - When a consumer joins a group, it is handed partitions normally.  However, when it leaves, it
        remains a member of the group until a session timeout.  If it comes back up before the timeout,
        it is reassigned the same partitions it had before, without triggering a rebalance.


    - If 2 consumers join a group with the same 'group.instance.id', the second one will get an error.


    - Static group membership is useful if your application maintains some local state or cache specific
        to individual partitions.  However, the partitions are blocked from consumption during the
        session timeout, so the consumer will have to catch up with the lag if it restarts.  Also,
        the session timeout needs to be chosen carefully.



- Creating a Kafka Consumer

    - To start consuming, we need to provide 3 mandatory properties:

        - bootstrap.servers
        - key.deserializer
        - value.deserializer

      Also, we can provide a 'group.id', which specifies the consumer group.  Creating consumers without
        a group is uncommon, so we will usually provide one.


    - To create a consumer:

        Properties props = new Properties();
        props.put("bootstrap.servers", "broker1:9092,broker2:9092");
        props.put("group.id", "CountryCounter");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);



- Subscribing to Topics

    - Once we create a consumer, we can subscribe to one or more topics.

        consumer.subscribe(Collections.singletonList("customerCountries"));


    - We can also use a regular expression to subscribe to a set of topics:

        consumer.subscribe(Pattern.compile("test.*"));



- Configuring Consumers

    - The consumer API uses a simple poll loop for polling the server for more data.

        Duration timeout = Duration.ofMillis(100);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("topic = %s, partition = %d, offset = %d, " 
                    + "customer = %s, country = %s\n",
                    record.topic(), 
                    record.partition(), 
                    record.offset(),
                    record.key(), 
                    record.value());

                int updatedCount = 1;

                if (custCountryMap.containsKey(record.value())) {
                    updatedCount = custCountryMap.get(record.value()) + 1;
                }

                custCountryMap.put(record.value(), updatedCount);

                JSONObject json = new JSONObject(custCountryMap);
                System.out.println(json.toString());
            }
        }


    - The 'poll()' method returns a list of records.  The 'timeout' controls how long it will block
        if data is not available.  If this is set to 0 or there are records available, it returns
        immediately.  Otherwise, it will wait the specified ms.  A consumer must keep polling or it
        will be considered dead.


    - Processing usually ends in writing to a data store or updating a stored record.  Here, we are
        keeping a running count of customers from each country, so we update a hash table and print
        the result as JSON.  A more realistic example would update results in a data store.


    - The first time a consumer polls is when they get a partition assignment.  If a rebalance is
        triggered, it will happen inside the poll loop as well.


    - If a poll is not invoked for longer than 'max.poll.interval.ms', the consumer will be considered
        dead and evicted from the consumer group.  So, we should avoid doing anything that can block
        for unpredictable intervals in the poll loop.


    - For safety reasons, we should only have one consumer per thread.  If we want to use multiple
        threads, each should have it's own consumer.



- Configuring Consumers

    fetch.min.bytes                         # Min amount of data to receive in a single poll

    fetch.max.wait.ms                       # How long to wait if you're waiting for min amount of data

    fetch.max.bytes                         # Max bytes in a single poll

    max.poll.records                        # Max number of records in a single poll

    max.partition.fetch.bytes               # Max bytes per partition

    session.timeout.ms                      # Max time with no communication before consumer considered dead

    heartbeat.interval.ms                   # How frequent to set heartbeat

    max.poll.interval.ms                    # Max time with no poll before consumer considered dead

    default.api.timeout.ms                  # Max time before API calls (not poll) time out

    request.timeout.ms                      # Max time consumer waits for response from broker

    auto.offset.reset                       # Default offset to read from at startup (default: latest)

    enable.auto.commit                      # Whether consumer will commit offsets automatically

    partition.assignment.stragegy           # Range(default), RoundRobin, Sticky, or Cooperative Sticky

    client.id                               # Can be any string, used for logging and quotas

    client.rack                             # Used to enable closest fetching

    group.instance.id                       # Any unique string, used for static membership

    receive.buffer.bytes                    # Sizes of TCP send and receive buffers
    send.buffer.bytes

    offsets.retention.minutes               # Broker configuration for retention



- Commits and Offsets

    - When we call 'poll()', it returns records we have not read yet.  To track what we have read,
        consumers use Kafka to keep track of their offset in each partition.


    - We call the action of updating the current position in the partition an 'offset commit'.  Consumers
        commit the last message they've successfully processed from a partition and implicitly assume
        every message before was also successfully processed.


    - A consumer commits an offset by sending a message to Kafka, which updates a '_consumer_offsets'
        topic with the committed offset for each partition.


    - If a consumer exits or enters the consumer group, a rebalance is triggered.  In order to know
        where to pick up, the consumer will read the latest committed offset from each of the partitions
        it's assigned an pick up from there.


    - If the committed offset is smaller than the offset of the last message processed, the messages
        between them will be processed twice.

      If the committed offset is larger than the offset of the last message processed, the messages
        between them will be missed.


    - By default, the committed offset is the last offset that was returned by 'poll()'.  Since this has
        such a large impact on client applications, there are several ways to manage it.



- Automatic Commit

    - The easiest way to commit offsets is to allow the consumer to do it for you.  If you enable
        'enable.auto.commit=true', then every 5 seconds, the consumer will commit the latest offset that
        your client received from 'poll()'.  That interval can be set with 'auto.commit.interval.ms'.


    - Just like other consumer things, automatic commits are driven by the poll loop.  Whenever you poll,
        the consumer checks if it's time to commit, and will commit if it is.


    - Note that this method leaves open the possibility of duplicated processing.


    - Like the 'poll()' method, calling the 'close()' method (when the poll loop is exited) also commits
        offsets automatically.



- Commit Current Offset

    - We can also commit the current offset to eliminate the possibility of duplicated or missed messages.
        In this case, we set 'enable.auto.commit=false', and the offsets will only be committed when the
        application explicitly does so.


    - The simplest and most reliable way to do this is to call 'commitSync()'.  An exception will be
        thrown if the commit fails for some reason.


    - Remember that 'commitSync()' commits the latest offset returned by 'poll()', so if you call
        'commitSync()' before you process all the records, you risk missing messages if the application
        crashes.

      If the call 'commitSync()' after you process all the messages, and the application crashes during
        processing, you risk duplicated messages.


    - Here, we commit offsets after processing the latest batch of messages:

        Duration timeout = Duration.ofMillis(100);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("topic = %s, partition = %d, offset =
                    %d, customer = %s, country = %s\n",
                    record.topic(),
                    record.partition(),
                    record.offset(),
                    record.key(),
                    record.value());
            }

            try {
                consumer.commitSync();
            } catch (CommitFailedException e) {
                log.error("commit failed", e)
            }
        }



- Asynchronous Commit

    - One drawback of manual commit is that the application is blocked until the broker resonds to the
        commit request.  This will limit the throughput of the application.  Another option is to use
        asynchronous commit.  We just send the commit request and continue on:

        Duration timeout = Duration.ofMillis(100);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("topic = %s, partition = %s,
                    offset = %d, customer = %s, country = %s\n",
                    record.topic(),
                    record.partition(),
                    record.offset(),
                    record.key(),
                    record.value());
            }

            consumer.commitAsync();
        }


    - The drawback is that 'commitAsync()' will not retry if it fails.  This is because we can't have
        commits coming in out of order.  We can use a callback with 'commitAsync()' to log errors, but
        we should be very careful when using it for retries.

        consumer.commitAsync(new OffsetCommitCallback() {
            public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception e) {
                if (e != null)
                    log.error("Commit failed for offsets {}", offsets, e);
            }
        });


    - If we did want to retry, we could keep track of the largest offset in our application, and only
        monotonically increase it.



- Combining Synchronous and Asynchronous Commits

    - One compromise is to make asynchronous commits, but make a synchronous call before we close a
        consumer or rebalance.

        try {
            while (!closing) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);

                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("topic = %s, partition = %s, offset = %d,
                        customer = %s, country = %s\n",
                        record.topic(),
                        record.partition(),
                        record.offset(),
                        record.key(),
                        record.value());
                }
                consumer.commitAsync();
            }
            consumer.commitSync();
        } catch (Exception e) {
            log.error("Unexpected error", e);
        } finally {
            consumer.close();
        }



- Committing a Specified Offset

    - Committing the latest offset only allows you to commit as often as you finish processing batches.
        We may want to commit more frequently, for instance if 'poll()' returns a huge batch and we want
        to avoid reprocessing if a rebalancing occurs.


    - The 'commitSync()' and 'commitAsync()' allow you to pass a map of partitions and offsets you want
        to commit.  Here, we commit offsets every 1000 records:


        private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
        int count = 0;
        ....
        Duration timeout = Duration.ofMillis(100);

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(timeout);

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("topic = %s, partition = %s, offset = %d,
                    customer = %s, country = %s\n",
                    record.topic(),
                    record.partition(),
                    record.offset(),
                    record.key(),
                    record.value());

                currentOffsets.put(
                    new TopicPartition(record.topic(), record.partition()),
                    new OffsetAndMetadata(record.offset()+1, "no metadata"));

                if (count % 1000 == 0)
                    consumer.commitAsync(currentOffsets, null);
                    count++;
            }
        }



- Rebalancing Listeners

    - If you know your consumer is about to lose ownership of a partition, you will want to commit the
        offset of the last message you processed.  You may also want to close file handles or database
        connections, etc.


    - The Consumer API allows you to receive notifications so you can do this.  You pass a 
        'ConsumerRebalanceListener' when calling the 'subscribe()' method, and you can implement these
        3 methods:

        # Called after partitions have been reassigned but before consuming starts
        public void onPartitionsAssigned(Collection<TopicPartition> partitions)

        # Called when consumer has to give up partitions it previously owned
        public void onPartitionsRevoked(Collection<TopicPartition> partitions)

        # Only called when a cooperative rebalancing algorithm is used
        public void onPartitionsLost(Collection<TopicPartition> partitions)



- Consuming Records with Specific Offsets

    - So far, we have used 'poll()' to start consuming messages from the last committed offset in each
        partition and to proceed in processing all messages in sequence.  We can also use a varity of
        methods to start consuming at a different offset.


    - We can use the 'seekToBeginning' method to start reading all messages from the beginning of the
        partition.  We can use 'seekToEnd' to start consuming only new messages.


    - We can also seek a specific offset.  For instance, a consumer could be reset back to a specific
        point in time in order to recover data that was lost.


    - Here, we set the current offset on all partitions to records that were produced at a specific
        point in time:

        Long oneHourEarlier = Instant.now().atZone(ZoneId.systemDefault()).minusHours(1).toEpochSecond();

        Map<TopicPartition, Long> partitionTimestampMap = consumer.assignment()
            .stream()
            .collect(Collectors.toMap(tp -> tp, tp -> oneHourEarlier));

        Map<TopicPartition, OffsetAndTimestamp> offsetMap = consumer.offsetsForTimes(partitionTimestampMap);

        for(Map.Entry<TopicPartition,OffsetAndTimestamp> entry: offsetMap.entrySet()) {
            consumer.seek(entry.getKey(), entry.getValue().offset());
        }



- But How Do We Exit?

    - When you decide to shut down the consumer, and you want to exit immediately even though the 
        consumer may be waiting on a longer poll(), you need another thread to call 'consumer.wakeup()'.
        If you are running the consumer loop in the main thread, this can be done from the 'ShutdownHook'.


    - Note that 'consumer.wakeup()' is the only consumer method that is safe to call from a different
        thread.  Calling 'wakeup()' will cause 'poll()' to exit with 'WakeupException'.


    - The 'WakeupException' doesn't need to be handled, but before exiting the thread, you must call
        'consumer.close()', which will commit offsets if needed and send a message to the GroupCoordinator
        that the consumer is leaving the group.


    - This is a truncated example:

        Runtime.getRuntime().addShutdownHook(new Thread() {
            public void run() {
                System.out.println("Starting exit...");
                consumer.wakeup();
                try {
                    mainThread.join();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        });

        Duration timeout = Duration.ofMillis(10000);

        try {
            // looping until ctrl-c, the shutdown hook will cleanup on exit
            while (true) {
                ConsumerRecords<String, String> records = movingAvg.consumer.poll(timeout);
                System.out.println(System.currentTimeMillis() + "-- waiting for data...");

                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("offset = %d, key = %s, value = %s\n",
                        record.offset(), record.key(), record.value());
                }

                for (TopicPartition tp: consumer.assignment())
                    System.out.println("Committing offset at position:" + consumer.position(tp));

                movingAvg.consumer.commitSync();
            }
        } catch (WakeupException e) {
            // ignore for shutdown
        } finally {
            consumer.close();
            System.out.println("Closed consumer and we are done");
        }



- Deserializers

    - Deserializers are required to convert byte arrays received from Kafka into PL objects.  For instance,
        we can use the simple 'StringDeserializer' to deserialize strings.


    - Obviously, the serializer used to produce events must match the deserializer used to consume them.
        This means that developers must keep track of which serializers were used to write, and must
        ensure that each topic only contains messages your deserializers can interpret.


    - This is one of the benefits of using Avro and the Schema Registry for serializing and deserializing.
        The 'AvroSerializer' can make sure that all the data written to a specific topic is compatible
        with the schema of the topic, which means it can be deserialized.  Any errors will be caught
        on the Producer side.


    - We can write a custom deserializer for our 'Customer' class similarly to how we wrote the serializer.
        Again, this is error-prone and not recommended.  A better solution is to use a well-known
        standard message format.



- Using Avro Deserialization with Kafka Consumer

    - To read the 'Customer' class that was described previously:

        Duration timeout = Duration.ofMillis(100);

        Properties props = new Properties();
        props.put("bootstrap.servers", "broker1:9092,broker2:9092");
        props.put("group.id", "CountryCounter");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "io.confluent.kafka.serializers.KafkaAvroDeserializer");
        props.put("specific.avro.reader","true");
        props.put("schema.registry.url", schemaUrl);

        String topic = "customerContacts";
        KafkaConsumer<String, Customer> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(topic));

        System.out.println("Reading topic:" + topic);

        while (true) {
            ConsumerRecords<String, Customer> records = consumer.poll(timeout);

            for (ConsumerRecord<String, Customer> record: records) {
                System.out.println("Current customer name is: " + record.value().getName());
            }

            consumer.commitSync();
        }



- Standalone Consumer: Why and How to Use a Consumer Without a Group

    - Sometimes, you just need a single consumer to read all the data in a topic, and you don't need a
        group.  In this case, you can just add a consumer and specify which partitions you want to read.


    - Here, a consumer can assign itself all partitions of a specific topic and consume from them.

        Duration timeout = Duration.ofMillis(100);

        List<PartitionInfo> partitionInfos = null;
        partitionInfos = consumer.partitionsFor("topic");

        if (partitionInfos != null) {
            for (PartitionInfo partition : partitionInfos)
                partitions.add(new TopicPartition(partition.topic(), partition.partition()));

            consumer.assign(partitions);

            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(timeout);

                for (ConsumerRecord<String, String> record: records) {
                    System.out.printf("topic = %s, partition = %s, offset = %d,
                        customer = %s, country = %s\n",
                        record.topic(), 
                        record.partition(), 
                        record.offset(),
                        record.key(), 
                        record.value());
                }

                consumer.commitSync();
            }
        }