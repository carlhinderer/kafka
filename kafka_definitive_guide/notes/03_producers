----------------------------------------------------------------------------
|  CHAPTER 3 - KAFKA PRODUCERS                                             |
----------------------------------------------------------------------------

- Kafka Producers

    - In this chapter, we'll look at Kafka's built-in 'KafkaProducer' and 'ProducerRecord' objects.
        In addition to these built-in clients, Kafka has a binary wire protocol, so many third party
        clients have been created as well.


    - Common Use Cases for Producers

        - Recording user activities for auditing or analysis
        - Recording metrics
        - Storing log messages
        - Recording information from smart appliances
        - Communicating asynchronously with other applications
        - Buffering information before writing to an application


    - For every use case we should ask ourselves:

        1. Is every message critical, or can we tolerate loss of messages?
        2. Are we OK with duplicated messages?
        3. Are there strict latency or throughput requirements?


    - For instance, we are sending credit card processing messages.  We cannot lose messages or tolerate
        duplicated messages.  Throughput will be high (1 million messages per s) and latency should be
        under 500 ms.


    - Another use case is storing click events from a website.  In this case, some message loss or
        duplication is tolerable.  Latency can be high as long as it doesn't affect the user experience.



- High-Level Producer Flow

    1. We start producing messages by creating a 'ProducerRecord', which must have a topic and value.
         Optionally, we can also specify a key, a partition, a timestamp, and/or a collection of 
         headers.

       Once we send the ProducerRecord, the producer will serialize the key and value objects to byte
         arrays so they can be sent over the network.


    2. Next, if we didn't explicitly specify a partition, the data is sent to a partitioner.  The
         partitioner will choose a partition for us, usually based on the 'ProducerRecord' key.

       Once a partition is selected, the producer knows which topic and partition the record will go
         to.  It adds the message to a batch that will go to the same topic and partition.  A separate
         thread is responsible for sending those batches to the appropriate Kafka brokers.


    3. When the broker retrieves the messages, it sends back a response.  If the messages were written
         successfully, it will return a 'RecordMetadata' object with the topic, partition, and
         offset of the record within the partition.

       If the broker failed to write the messages, it will return an error.  When the producer receives
         an error, it may retry sending the message a few more times before giving up.



                                ProducerRecord[Topic, Value, (Partition), (Key)]
                                                      |
                                                    Send()
                                                      |
                                                      V
                                                  Serializer
                                                      |
                                                      V
                             ^                   Partitioner
                             |                        |
                           Exception       -------------------------
                             |             |                       |
                            No             V                       V
                             |         Topic A                  TopicB
                     ^       |  Yes--->Partition 0              Partition 1
                     |       |   |     ------------             ------------
                  Return     Retry?    Batch 0                  Batch 0
                Metadata       |       Batch 1                  Batch 1
                     |        Yes      Batch 2                  Batch 2
                     |         |          |                       |
                     No------Fail?        -------------------------
                               |                      |
                               |                      V
                               |-----------------Kafka Broker




- Constructing a Kafka Producer

    - First, we need to create a producer object with the properties we want to pass to the producer.
        There are 3 mandatory properties:

        bootstrap.servers                  # List of host:port pairs of brokers the producer will use
                                           #   to establish initial connection to the cluster
                                           #   (Does not need to include all brokers, but should include 2)

        key.serializer                     # Name of class used to serialize keys

        value.serializer                   # Name of class used to serialize values


    - The standard Kafka serializers are:

        ByteArraySerializer                # Doesn't do much
        StringSerializer
        IntegerSerializer



    - Note that you still have to set the 'KeySerializer' even if you only intend to send values.  In
        that case, you can use the 'VoidSerializer'.


    - Here is an example of the most basic configuration:

       Properties kafkaProps = new Properties();

       kafkaProps.put("bootstrap.servers", "broker1:9092,broker2:9092");
       kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
       kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

       producer = new KafkaProducer<String, String>(kafkaProps);



- Sending a Message to Kafka

    - Now that we have instantiated a producer, we can start sending messages.  There are 3 primary 
        methods of sending messages:

        1. Fire-and-forget = We send a message, and don't really care whether it arrives successfully
                               or not.

        2. Synchronous send = Technically, a Kafka producer is always asynchronous - we send a message,
                                and the 'send()' method returns a 'Future' object.  However, we use
                                'get()' to wait on the 'Future' before sending the next record.

        3. Asynchronous send = We call the 'send()' method with a callback function, which gets
                                 triggered when it receives a response from the Kafka broker.


    - Here is the simplest way to send a message:

        ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");

        try {
            producer.send(record);
        } catch (Exception e) {
            e.printStackTrace();
        }

      We are using fire-and-forget here, and we're ignoring any errors we receive back from Kafka.
        The exceptions we are catching are thrown before sending the message to Kafka.



- Sending a Message Synchronously

    - Sending a message synchronously is simple, and allows the producer to catch exceptions when
        Kafka returns an error or send retries are exhausted.  The tradeoff is a performance hit from
        blocking.


    - To send a message synchronously:

        ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");

        try {
            producer.send(record).get();
        } catch (Exception e) {
            e.printStackTrace();
        }


    - 'KafkaProducer' has 2 types of errors:

        1. 'Retriable' errors that can be resolved by sending the message again
        2. Errors that cannot be resolved by retry like the message being too big

      The KafkaProducer can be configured to retry automatically if a retriable error is received.



- Sending a Message Asynchronously

    - Blocking synchronously on every message sent will cause a drastic performance penalty.  On the
        other hand, if we just send all our messages without waiting for replies, it will be much
        faster.

      In most cases, we don't actually need a reply.  Kafka sends back the topic, partition, and offset
        of the record that was written.  Most applications don't need this information.


    - On the other hand, we do need to know when we failed to send a message completely, so we can throw
        an exception, log an error, or write to an 'errors' file for later analysis.


    - To send messages asynchronously and still handle error scenarios, we can use callbacks.

        private class DemoProducerCallback implements Callback {
            @Override
            public void onCompletion(RecordMetadata recordMetadata, Exception e) {
                if (e != null) {
                    e.printStackTrace();
                }
            }
        }

        ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Biomedical Materials", "USA");

        producer.send(record, new DemoProducerCallback());



- Configuring Producers

    - Configuration parameters for producers:

        client.id              # Logical id for client, makes log tracing much easier, can be any string

        acks                   # Number of replicas that must receive a record before write is
                               #   considered successful
                               #
                               #   acks=0             # Fire and forget
                               #   acks=1             # Leader receives message (default)
                               #   acks=all           # All in sync replicas receive the message


    - Note that the 'acks' is a tradeoff for Producer latency, not end-to-end latency.  In order to
        maintain consistency, Kafka will not allow consumers to read records until they are written to
        all in-sync replicas.  So the end-to-end latency is the same whether 'acks' is 0, 1, or 'all'.



- Message Delivery Configuration

    - The Producer has multiple configuration parameters that control how much time to wait until
        we give up on a 'send()' call.

      
    - Since Kafka 2.1, we divide the time spent sending a ProducerRecord into 2 time intervals that
        are handled separately:

        1. Time until an async call to 'send()' returns.  During this interval, the thread that called
             'send()' will be blocked.

        2. From the time an async call to 'send()' returned successfully until the callback is triggered
             (success or failure).


          max.block.ms
        |----------------|  linger.ms
                         |-------------|
                                       |---------|  retry.backoff.ms
                                                 |--------------------|  request.timeout.ms
                                                                      |-----------------------|

                                                delivery.timeout.ms 
                         |--------------------------------------------------------------------|


        |     send()     |  Batching   | Await   |   Retries          |  Inflight             |
                                         send



    - Note that: delivery.timeout.ms >= (linger.ms + retry.backoff.ms + request.timeout.ms)


    - Here is a description of the delivery config parameters:

        max.block.ms           # Max amount of time to block on send(), exception is thrown when reached

        delivery.timeout.ms    # Max amount of time to spend sending and retrying

        request.timeout.ms     # Max amount of time to wait on reponse from server on a single try

        retries                # Number of retries

        retry.backoff.ms       # Time to wait between retries (100 ms by default)


    - In general, there is no point in handling retries in application logic, since the Producer does
        it for you.  Focus your efforts on handling nonretriable errors or cases where retry attempts
        were exhausted.


    - Some cases are extremely sensitive to ordering guarantees.  For instance, you want a message
        depositing $100 to arrive before a message withdrawing $100.

      Kafka guarantees ordering in a partition, but retries could cause events to arrive out of order.
        If you need to guarantee order, the best way is to send 'enable_idempotence=true'.  This
        guarantees order for up to 5 in-flight requests, and also guarantees retries will not
        introduce duplicates.



- Other important configuration parameters:

        linger.ms              # Amount of time to wait for additional messages before sending batch

        buffer.memory          # Amount of memory to use buffering messages before sending

        compression.type       # Can use snappy, gzip, lz4, or zstd (none by default)

        batch.size             # Amount of memory (in bytes) used for each batch before sending

        max.in.flight.requests.per.connection   # Number of messages producer will send to server 
                                                #   without receiving responses

        max.request.size       # Maximum size of entire batch of messages

        receive.buffer.bytes   # Sizes of TCP send and receive buffers used by sockets when sending
        send.buffer.bytes      #   and receiving data

        enable.idempotence     # Used to support exactly-once semantics, producer will attach 
                               #   sequence number that Kafka will use to reject duplicates
                               #
                               # Note that enabling this also requires:
                               #   max.in.flight.requests.per.connection = 5 or less
                               #   retries = 1 or greater
                               #   acks = all



- Custom Serializers

    - Kafka has several built-in serializers, including for Strings, integers, and ByteArrays.
        However, eventually you will want to serialize more complex objects.


    - When you want to send an object that isn't a simple string or integer, you can either use a
        serialization library (ie Avro, Thrift, Protobuf) or create a custom serialization.


    - Suppose that instead of just recording the customer name, you want to create a class to
        represent customers:

        public class Customer {
            private int customerID;
            private String customerName;

            public Customer(int ID, String name) {
                this.customerID = ID;
                this.customerName = name;
            }

            public int getID() {
                return customerID;
            }

            public String getName() {
                return customerName;
            }
        }


    - Now, we can create a custom serializer for the class:

        import org.apache.kafka.common.errors.SerializationException;

        import java.nio.ByteBuffer;
        import java.util.Map;

        public class CustomerSerializer implements Serializer<Customer> {

            @Override
            public void configure(Map configs, boolean isKey) {
                // nothing to configure
            }

            @Override
            /**
            We are serializing Customer as:
            4 byte int representing customerId
            4 byte int representing length of customerName in UTF-8 bytes (0 if name is Null)
            N bytes representing customerName in UTF-8
            **/
            public byte[] serialize(String topic, Customer data) {
                try {
                    byte[] serializedName;
                    int stringSize;
                    if (data == null)
                        return null;
                    else {
                        if (data.getName() != null) {
                            serializedName = data.getName().getBytes("UTF-8");
                            stringSize = serializedName.length;
                        } else {
                            serializedName = new byte[0];
                            stringSize = 0;
                        }
                    }

                    ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + stringSize);
                    buffer.putInt(data.getID());
                    buffer.putInt(stringSize);
                    buffer.put(serializedName);

                    return buffer.array();
                } catch (Exception e) {
                    throw new SerializationException("Error when serializing Customer to byte[] " + e);
                }
            }

            @Override
            public void close() {
                // nothing to close
            }
        }


    - There are lots of reasons to not actually implement the serialization yourself.  The code is
        brittle, versioning will be nearly impossible, and debugging is challenging.  We should use a
        common library such as JSON, Avro, Thrift, Protobuf instead.



- Serializing Using Apache Avro

    - Avro was created by Doug Cutting as a way to share large data files with a large audience.

        - The schema is usually described in JSON
        - Serialization is usually to binary files, although serialization to JSON is also supported
        - Avro assumes the schema is present when reading and writing files, usually embedded in files


    - When the application that is writing messages switches to a new but compatbile schema, the readers
        can continue processing messages without requiring any updates.


    - Suppose the original schema was:

        {
            "namespace": "customerManagement.avro",
             "type": "record",
             "name": "Customer",
             "fields": [
                 {"name": "id", "type": "int"},
                 {"name": "name", "type": "string"},
                 {"name": "faxNumber", "type": ["null", "string"], "default": "null"}
             ]
        }

      The 'id' and 'name' fields are mandatory, while 'faxNumber' is optional and defaults to null.


    - Now, we'll update our schema, deciding that a fax number is no longer necessary and including an
        email field instead:


        {
            "namespace": "customerManagement.avro",
            "type": "record",
            "name": "Customer",
            "fields": [
                {"name": "id", "type": "int"},
                {"name": "name", "type": "string"},
                {"name": "email", "type": ["null", "string"], "default": "null"}
            ]
        }


    - When the reading application encounters a message using the new schema, 'getFaxNumber()' will 
        return null since it is not present.

      When the reading application encounters a message using the old schema, 'getEmail()' will return
        null since it is not present.


    - Note that there are compatability rules that must be followed for schemas to be compatible.  For
        backwards compatability, you can delete fields or add optional fields.  For forward compatability,
        you can add fields or delete optional fields.

      Thus, for full backwards and forwards compatibility, you can evolve schemas by:

        1. Adding optional fields
        2. Deleting optional fields


    - Optional fields have a default value if one is not provided.  They are often union types with null
        and a default value of null.



- Using Avro Records with Kafka

    - Unlike Avro files, where storing the entire schema with the file adds a reasonable amount of
        overhead, storing the schema with each record will usually more than double the record size.


    - To alleviate this, we use a Schema Registry.  In this case, we'll use Confluent Schema Registry.
        We store all the schemas used to write data into the registry.  Then, we add the id of the
        schema into the record when we produce to Kafka.


    - The consumers can then use the id to pull the record out of the Schema Registry and deserialize
        the data.  All this work is done in the serializers and deserializers, we shouldn't need to
        code for this.


    - This is an example of producing Avro objects to Kafka:

        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("value.serializer", "io.confluent.kafka.serializers.KafkaAvroSerializer");
        props.put("schema.registry.url", schemaUrl);

        String topic = "customerContacts";

        Producer<String, Customer> producer = new KafkaProducer<>(props);

        // We keep producing new events until someone ctrl-c
        while (true) {
            Customer customer = CustomerGenerator.getNext();
            System.out.println("Generated customer " + customer.toString());
            ProducerRecord<String, Customer> record = 
                new ProducerRecord<>(topic, customer.getName(), customer);
            producer.send(record);



- Partitions


- Headers


- Interceptors


- Quotas and Throttling