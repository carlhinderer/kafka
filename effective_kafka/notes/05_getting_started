-----------------------------------------------------------------------
|  CHAPTER 5 - GETTING STARTED                                        |
-----------------------------------------------------------------------

- Creating a Topic

    - We can create a topic from the command line using the 'kafka-topics.sh' script.

        $ $KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 \
             --create --partitions 3 --replication-factor 1 \
             --topic getting-started


    - We can also create a topic by clicking the 'New' button in Kafdrop.


    - We can set the 'auto.create.topics.enable' broker config setting to true, and we don't
        even have to manually create topics.  A topic will just get created automatically
        when clients attempt to produce to or consume from it.

      This is not advisable, though.  You might end up with stray topics due to typos, and
        you don't get control over the number of partitions or other settings.



- Publishing Records

    - Now, we'll publish a few records using the 'kafka-console-producer.sh' script.

        # Run to start producing
        $ $KAFKA_HOME/bin/kafka-console-producer.sh \
            --broker-list localhost:9092 \
            --topic getting-started --property "parse.key=true" \
            --property "key.separator=:"

        # Now, create the new records, which are separated by newlines
        foo:first message
        foo:second message
        bar:first message
        foo:third message
        bar:second message

        # Press Cntl+D when done



- Consuming Records

    - Now, we'll consume some records with the 'kafka-console-consumer.sh' script.

        # Consume the records
        $ $KAFKA_HOME/bin/kafka-console-consumer.sh \
             --bootstrap-server localhost:9092 \
             --topic getting-started --group cli-consumer --from-beginning \
             --property "print.key=true" --property "key.separator=:"

        # The terminal will output
        bar:first message
        bar:second message
        foo:first message
        foo:second message
        foo:third message

        # The consumer will continue to tail the topic, Press Cntl+D to stop it


    - Note that we used the '--from-beginning' flag.  By default, a first-time consumer will
        have its offsets reset to the topic's high-water mark.  We override this offset to tail
        the topic's low-water mark.

      To start from the end of the topic, just delete the consumer offsets and start the CLI
        consumer with no flag.


    - In Kafdrop, we can navigate to the topic and we'll see our new consumer.  If we click on
        it, we'll see the offset for each partition.



- Listing Topics

    - We can list the topics with the 'kafka-topics.sh' script.

        $ $KAFKA_HOME/bin/kafka-topics.sh \
            --bootstrap-server localhost:9092 \
            --list --exclude-internal



- Describing a Topic

    - We can also get details about a topic with the 'kafka-topics.sh' script.

        $ $KAFKA_HOME/bin/kafka-topics.sh \
            --bootstrap-server localhost:9092 \
            --describe --topic getting-started



- Deleting a Topic

    - We can also delete a topic using the 'kafka-topics.sh' script.

        $ $KAFKA_HOME/bin/kafka-topics.sh \
            --bootstrap-server localhost:9092 \
            --topic getting-started --delete


    - Topic deletion is an asynchronous operation, which will be performed by a 
        background process some time in the future.  In the meantime, it may seem to 
        linger around.

      This can require some creativity when writing integration tests, for example.  The
        most common way around this is to use a unique topic for each test, to avoid
        topics interfering with other tests.



- Truncating Partitions

    - Although a partition is backed by an immutable log, Kafka has a mechanism to
        truncate all records in the log up to a user-specified low-water mark.

      This can be achieved by passing a JSON document to the 'kafka-delete-records.sh' tool, 
        specifying the topics and partitions for truncation, with the new low-watermark in the
        'offset' attribute.


    - Here, we create a json document and pass it into the 'kafka-delete-records.sh' script.

        cat << EOF > /tmp/offsets.json
        {
          "partitions": [
            {"topic": "getting-started", "partition": 2, "offset": 1}
          ],
          "version": 1
        }
        EOF
    
        $ $KAFKA_HOME/bin/kafka-delete-records.sh \
            --bootstrap-server localhost:9092 \
            --offset-json-file /tmp/offsets.json



- Listing Consumer Groups

    - The 'kafka-consumer-groups.sh' script can be used to query Kafka for a list of consumer
        groups.

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
          --bootstrap-server localhost:9092 --list


    - The result is a newline-separated list of consumer group names, which makes it easy for
        us to iterate over them for administrative purposes.

        #!/bin/bash

        list_groups_cmd="$KAFKA_HOME/bin/kafka-consumer-groups.sh \
          --bootstrap-server localhost:9092 --list"

        forgroup in$(bash -c $list_groups_cmd); do
          # do something with the $group variable
        done



- Describing a Consumer Group

    - The 'kafka-consumer-groups.sh' script can also be used to display detailed state information
        about each consumer group, including its partition offsets for the set of subscribed
        topics.

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --group cli-consumer --describe --all-topics


    - To describe all groups:

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --describe --all-groups --all-topics


    - To get details about the present state of the consumer group, including the ID of the
        coordinator node, the assignment strategy, the number of active members, and the
        state of the group, the '--state' flag is used.

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --describe --all-groups --state



- Resetting Offsets

    - We may encounter situations where we need to reset offsets.  An example is if we realize
        a set of records contains erroneous data ('poisoned records').  Another example is if
        we need to reprocess earlier records.


    - To reset the offset to the low-water mark, we use the '--to-earliest' flag.  To reset the
        offset to the high-water mark, we use the '--to-latest' flag is used.  Note that the
        command will result in a 'dry run' unless the '--execute' flag is used.

        # Reset offset to low-water mark
        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --topic getting-started --group cli-consumer \
            --reset-offsets --to-earliest --execute


    - In addition to resetting offsets for an entire topic, the reset operation can be performed
        on a subset of a topic's partitions.

        # Reset offset for partitions 0 and 1
        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --topic getting-started:0,1 --group cli-consumer \
            --reset-offsets --to-offset 2 --execute


    - This example uses Kafka's record timestamping to locate an offset based on a given
        datetime value.  The offset will be reset to the earliest point that occurs at or after
        the specified timestamp.

        # Datetime value in ISO 8601 format
        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --topic getting-started:2 --group cli-consumer \
            --reset-offsets --to-datetime2020-01-27T14:35:54.528+11:00 \
            --execute


    - Finally, we can shift by a fixed number of records using the '--shift-by' parameter.
        We can shift forward (positive value) or backward (negative value).



- Deleting Offsets

    - Another method of resetting the offsets is to just delete them altogether.  This is
        in effect a lazy reset, sine the assignment of new offsets does not occur until a
        new consumer connects to the cluster.

      When this happens, the 'auto.offset.reset' client property determines whether the
        offset is reset to the earliest or latest offset.

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --topic getting-started --group cli-consumer --delete-offsets



- Deleting a Consumer Group

    - To delete an entire consumer group and its associated state:

        $ $KAFKA_HOME/bin/kafka-consumer-groups.sh \
            --bootstrap-server localhost:9092 \
            --group cli-consumer --delete