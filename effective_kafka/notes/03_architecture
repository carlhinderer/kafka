-----------------------------------------------------------------------
|  CHAPTER 3 - ARCHITECTURE & CORE CONCEPTS                           |
-----------------------------------------------------------------------

- Architecture Overview

    1. Broker nodes = responsible for I/O operations and durable persistence

    2. ZooKeeper nodes = manages the cluster status, provides consistent state repository
                           that can be shared by brokers

    3. Producers = client applications that append records to topics

    4. Consumers = client applications that read from topics



- Broker Nodes

    - Note that the terms 'Kafka server', 'Kafka broker', and 'Kafka node' are used 
        interchangeably.


    - A Kafka broker is a Java process that acts as part of a larger cluster.  The minimum
        cluster size is 1.


    - A broker fulfills its persistence obligations by hosting a set of append-only log files
        that comprise the partitions hosted by the clusters.


    - Each partition is mastered by exactly one broker - the partition 'leader'.  Partition data
        is replicated to a set of 0 or more 'follower' brokers.  Collectively, the leader and
        followers are referred to as 'replicas'.  A broker may be a leader for one partition and
        a follower for others.


    - Broker nodes are largely identical in every way.  Each node competes for mastership of a
        partition on equal footing with its peers.  A single node is elected 'cluster controller',
        which directs all nodes (including itself) to assume specific roles.  It is the controller's
        resposibility to manage the states of partitions and replicas, and perform administrative
        tasks like reassigning partitions among the broker nodes.



- ZooKeeper Nodes

    - The election of the cluster controller is performed by ZooKeeper.  ZooKeeper itself is
        a cluster of cooperating processes called an 'ensemble'.  


    - A ZooKeeper ensemble acts as a consistent and highly available config repository,
        maintaining cluster metadata, leader-follower states, quotas, user information, access
        control lists, and other housekeeping items.  Due to the underlying consensus protocol
        used, the number of ZooKeeper nodes must be odd.


    - ZooKeeper ensures that only one broker node will be assigned controller status, and if that
        node fails of leaves the cluster, another broker node will take its place.



- Producers

    - A producer is a source of data in a Kafka cluster.  A producer communicates with a
        cluster over a set of persistent TCP connections, with an individual connection
        established with each broker.


    - Producers can publish records to one or more Kafka topics, and any number of producers
        can append records to the same topic.  Consumers cannot modify topics in any way.



- Consumers

    - A consumer is a client application that acts as a data sink, subscribing to streams of
        records from one or more topics.


    - Consumers are more complex than producers, since they have to coordinate among themselves
        to balance the load of consuming records and track their progress through the stream.



- Total and Partial Order

    - A 'totally ordered set' is one where every element has a well-defined ordering 
        relationship with every other element in the set.

        1 -> 2 -> 3 -> 4 -> 5

      We can remove elements from the set and re-add them, and we will always arrive at the
        same sequence.


    - An 'unordered set' has no implicit order.

        [Sydney, New York, London]


    - A 'partially ordered set' indicates that for certain pairs of elements in the set, one
        of the pair precedes the other in the ordering.  However, not every pair of elements
        needs to be comparable.

      For example, in this set, a number must appear after its divisor.

        [2, 3, 5, 7, 11, 4, 6, 9, 10, 8, 12]

      Or for another example, if we have 2 alphabets, English {A, B, C, ..., Z} and 
        Cyrillic {А, Б, В, ..., Я}, their union would be partially ordered if each letter
        maintained relative order within its own alphabet.



- Causal Order

    - The notion of 'causal order' comes from modern distributed systems theory.  One
        challenge of constructing such systems is that a message sent from one process to 
        another can arrive 0 or more times at any time after they are sent.


    - As a consequence, there is no agreeable notion of time among collaborating processes. If 
        one process, such as a clock, sends a timestamped message to another process, there is no 
        reliable way for a receiver to determine the time relative to the clock and synchronise the 
        two processes. This is often cited as the sole reason that building distributed systems is 
        hard.


    - In the absence of a global clock, the relative timing of a pair of events may be
        indistinguishable to an outside observer.  But, if they are causally related, they
        become comparable, so we know their order.


    - The original events themselves are not comparable, but the recorded observations of the 
        events are.  The observations are events in their own right.


    - For example, 2 samples of temperature readings R0 and R1 are taken at different sites.
        They are communicated to a remote receiver and recorded in the order of arrival, forming
        a causal relationship on the receiver.

      We can say that Received(R0) -> Received(R1).  However, this does not imply that 
        Sent(R0) -> Sent(R1).



- Records

    - A 'record' is the most elemental unit of persistence in Kafka.  In an EDA context, a
        record corresponds to some event of interest.


    - A record has the following attributes:

        Key

          A non-unique key, which acts as a classifier.  It is free-form - anything that can
            be represented as a sequence of bytes can be a key.

        Value

          The value is the informational payload of a record.  It describes the event.

        Headers

          A set of free-form key-value pairs that can optionally annotate a record.

        Partition Number

          A zero-based index of the partition that the record appears in.  A record is always
            tied to exactly one partition, but the partition doesn't need to be explicitly
            stated when the record is published.

        Offset

          A 64-bit signed integer for locating a record within its partition.  Records are
            stored sequentially.

        Timestamp

          A millisecond-precise timestamp of the record.  It can be set by either the producer
            or broker.


    - The 'key' attribute is not used like a primary key.  It is used as a non-unique classifier,
        and Kafka hashes the keys to map records to paritions.
        
      The primary key of a record is its partition number and offset.



- Partitions

    - A 'partition' is a totally ordered, unbounded set of records.  Published records are 
        appended to the head end of a partition.


    - Because records are totally ordered within their partition, any pair of records in the same 
       partition is bound by a predecessor-successor relationship. This relationship is implicitly 
       assigned by the producer application. 

      For any given producer instance, records will be written in the order they were emitted by the
        application. If record P was published before Q, then P will precede Q in the partition.

      Furthermore, they will be read in the same order by all consumers.  P will always be read before Q, 
        for every possible consumer.  This ordering guarantee is vital when implementing event-driven
        systems.


    - Records published to one partition by the same producer are causally ordered.  There is no 
        recognised causal ordering across producers.

      So, in the absence of producer synchronization, causal order can only be achieved when a 
        single producer emits records to the same partition.  Whether this matters depends on the
        application.


    - A record's offset uniquely identifies it in the partition.  This allows for fast, O(1) lookups.
        The offset is a strictly-increasing integer, although there may be gaps in the numbers, so
        we shouldn't try to guess what they will be.


    - A partition looks like:

            LOG END OFFSET     000015
                               000014
            HIGH WATER MARK    000013
                               000012   key | value | timestamp | headers
                               000011   key | value | timestamp | headers
                               000010   key | value | timestamp | headers
                               000009   key | value | timestamp | headers
                               000008   key | value | timestamp | headers
            LOW WATER MARK     000007   key | value | timestamp | headers
                               000006
                               000005


                - The next record will be written at the LOG END OFFSET.
                - The HIGH WATER MARK is aka the 'END OFFSET'.
                - The LOW WATER MARK is aka the 'BEGINNING OFFSET'.
                - Records below the LOW WATER MARK are truncated.

                - The LOW WATER MARK is the first record that will be presented to a consumer.
                    Due to bounded retention, this is not necessarily the first record that was
                    published.

                - Consumers are only allowed to read up to the HIGH WATER MARK.  This prevents 
                    cosumers from reading unreplicated data that may be lost in the event of a
                    leader failure.



- Topics

    - A 'topic' is a logical aggregation of partitions.  Topics allow for both parallelism and
        load balancing.


    - Since partitions exhibit total order, a topic exhibits partial order.  This allows us to 
        process records in parallel where we can, and maintain order where we must.


    - One way to think of a topic is as a wide stream, comprising multiple parallel substreams.
        The term 'stream' may be used as a substitute for 'topic'.


    - The producer decides how to partition records.  Sometimes, a producer will explicitly assign
        a partition number to a record.  Much more often, the producer will create a key/value, and
        will hash (using murmur2) the key modulo the number of partitions to get the partition 
        number.


    - A topic looks like:

        Topic

          Partition 0 |  007  008  009  010  011  012   <-|---PRODUCER 0
          Partition 1 |  000  001  002                  \-|---PRODUCER 1
          Partition 2 |  000  001  002  003             <-|---PRODUCER 2


    - Note that 2 records with the same key will be in the same partition only if the number of
        partitions has not changed.  Increasing the number of partitions (Kafka doesn't support
        non-destructive downsizing) will lead to a breakdown of any prior order.


    - Producers and consumers rarely care about partition numbers, only that related records are 
        grouped together in causal order.



- Consumer Groups and Load Balancing

    - There can be any number of producers and any number of consumers simultaneously interacting
        with a topic.


    - A 'consumer' is a process or thread that attaches to a Kafka cluster via a client library.
        A consumer generally (but not necessarily) becomes part of a 'consumer group'.  

      Consumer groups are a load-balancing mechanism in Kafka.  They distribute partition assignments
        evenly among the individual consumer instances in the group.

      When the first consumer in a group subscribes to a topic, it gets all the partitons in the
        topic.  When a second consumer subscribes, the partitions will be split in half.
        Each partition is assigned to only a single consumer.  If there are more consumers than 
        partitions, they will sit idle.


    - Note that consuming an event does not remove it from the topic.  It's actually more of a 
        'reader' than a 'consumer'.  In fact, a topic is an append-only log, and consumers have no
        effect on the topic.  This is one of the main differences between traditional message queues
        and event streams.


    - A consumer maintains a vector of offsets for each assigned partition, and advances it internally
        after each read.  Consumers read at their own pace - a backlogged consumer has no effect on
        its peers.


    - Consumer groups also ensure availability.  By periodically reading records from a topic, the
        consumer implicitly signals to the cluster that it is in a healthy state, thereby extending
        its lease on the partition assignment.

      If it fails to read in the allowable deadline, its partitions will be reassigned.
